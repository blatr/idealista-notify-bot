{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Idealista Scraper Debug - Scrapfly\n",
    "\n",
    "Using Scrapfly to bypass DataDome anti-bot protection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Scrapfly SDK\n",
    "!pip install scrapfly-sdk beautifulsoup4 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapfly import ScrapflyClient, ScrapeConfig, ScrapeApiResponse\n",
    "from bs4 import BeautifulSoup\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialize Scrapfly Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scrapfly client initialized!\n"
     ]
    }
   ],
   "source": [
    "SCRAPFLY_API_KEY = \"scp-live-1e9e5558c13049ccab83bc04ff5dab0f\"\n",
    "\n",
    "scrapfly = ScrapflyClient(key=SCRAPFLY_API_KEY)\n",
    "print(\"Scrapfly client initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build Search URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target URL:\n",
      "https://www.idealista.com/alquiler-viviendas/barcelona-barcelona/con-precio-hasta_2400,precio-desde_1000/\n"
     ]
    }
   ],
   "source": [
    "# Idealista Spain - Barcelona rentals\n",
    "SEARCH_URL = \"https://www.idealista.com/alquiler-viviendas/barcelona-barcelona/con-precio-hasta_2400,precio-desde_1000/\"\n",
    "\n",
    "print(f\"Target URL:\\n{SEARCH_URL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Scrape with Anti-Bot Bypass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CRITICAL:root:<-- 200 | ERR::SCRAPE::BAD_UPSTREAM_RESPONSE - The website you target respond with an unexpected status code (>400) - The scrapped url: https://www.idealista.com/alquiler-viviendas/barcelona-barcelona/con-precio-hasta_2400,precio-desde_1000/ respond with 403 - Forbidden: . Checkout the related doc: https://scrapfly.io/docs/scrape-api/error/ERR::SCRAPE::BAD_UPSTREAM_RESPONSE\n"
     ]
    },
    {
     "ename": "UpstreamHttpClientError",
     "evalue": "Target website responded with 403 - Forbidden",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUpstreamHttpClientError\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Make request with ASP (Anti Scraping Protection) enabled\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mscrapfly\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscrape\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mScrapeConfig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSEARCH_URL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43masp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Enable anti-bot bypass\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcountry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mES\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrender_js\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Render JavaScript\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStatus: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;241m.\u001b[39mupstream_status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSuccess: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;241m.\u001b[39msuccess\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/backoff/_sync.py:105\u001b[0m, in \u001b[0;36mretry_exception.<locals>.retry\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m details \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m: target,\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m\"\u001b[39m: args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melapsed\u001b[39m\u001b[38;5;124m\"\u001b[39m: elapsed,\n\u001b[1;32m    102\u001b[0m }\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 105\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exception \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    107\u001b[0m     max_tries_exceeded \u001b[38;5;241m=\u001b[39m (tries \u001b[38;5;241m==\u001b[39m max_tries_value)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/scrapfly/client.py:487\u001b[0m, in \u001b[0;36mScrapflyClient.scrape\u001b[0;34m(self, scrape_config, no_raise)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m no_raise \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, ScrapflyError) \u001b[38;5;129;01mand\u001b[39;00m e\u001b[38;5;241m.\u001b[39mapi_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    485\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m e\u001b[38;5;241m.\u001b[39mapi_response\n\u001b[0;32m--> 487\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/scrapfly/client.py:476\u001b[0m, in \u001b[0;36mScrapflyClient.scrape\u001b[0;34m(self, scrape_config, no_raise)\u001b[0m\n\u001b[1;32m    474\u001b[0m request_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scrape_request(scrape_config\u001b[38;5;241m=\u001b[39mscrape_config)\n\u001b[1;32m    475\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_http_handler(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrequest_data)\n\u001b[0;32m--> 476\u001b[0m scrape_api_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscrape_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscrape_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreporter\u001b[38;5;241m.\u001b[39mreport(scrape_api_response\u001b[38;5;241m=\u001b[39mscrape_api_response)\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m scrape_api_response\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/scrapfly/client.py:573\u001b[0m, in \u001b[0;36mScrapflyClient._handle_response\u001b[0;34m(self, response, scrape_config)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_handle_response\u001b[39m(\u001b[38;5;28mself\u001b[39m, response:Response, scrape_config:ScrapeConfig) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ScrapeApiResponse:\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 573\u001b[0m         api_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_api_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[43m            \u001b[49m\u001b[43mscrape_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscrape_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[43m            \u001b[49m\u001b[43mraise_on_upstream_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscrape_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_on_upstream_error\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m scrape_config\u001b[38;5;241m.\u001b[39mmethod \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHEAD\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    580\u001b[0m             logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<-- [\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m] \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (\n\u001b[1;32m    581\u001b[0m                 api_response\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code,\n\u001b[1;32m    582\u001b[0m                 api_response\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mreason,\n\u001b[1;32m    583\u001b[0m                 api_response\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39murl,\n\u001b[1;32m    584\u001b[0m                 \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    585\u001b[0m             ))\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/scrapfly/client.py:822\u001b[0m, in \u001b[0;36mScrapflyClient._handle_api_response\u001b[0;34m(self, response, scrape_config, raise_on_upstream_error)\u001b[0m\n\u001b[1;32m    812\u001b[0m         body \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    814\u001b[0m api_response:ScrapeApiResponse \u001b[38;5;241m=\u001b[39m ScrapeApiResponse(\n\u001b[1;32m    815\u001b[0m     response\u001b[38;5;241m=\u001b[39mresponse,\n\u001b[1;32m    816\u001b[0m     request\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mrequest,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    819\u001b[0m     large_object_handler\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_scrape_large_objects\n\u001b[1;32m    820\u001b[0m )\n\u001b[0;32m--> 822\u001b[0m \u001b[43mapi_response\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraise_on_upstream_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraise_on_upstream_error\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m api_response\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/scrapfly/api_response.py:608\u001b[0m, in \u001b[0;36mScrapeApiResponse.raise_for_result\u001b[0;34m(self, raise_on_upstream_error, error_class)\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(error, UpstreamHttpError):\n\u001b[1;32m    607\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m raise_on_upstream_error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 608\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    610\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\n",
      "\u001b[0;31mUpstreamHttpClientError\u001b[0m: Target website responded with 403 - Forbidden"
     ]
    }
   ],
   "source": [
    "# Make request with ASP (Anti Scraping Protection) enabled\n",
    "result = scrapfly.scrape(\n",
    "    ScrapeConfig(\n",
    "        url=SEARCH_URL,\n",
    "        asp=True,  # Enable anti-bot bypass\n",
    "        country=\"ES\",\n",
    "        render_js=True,  # Render JavaScript\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"Status: {result.upstream_status_code}\")\n",
    "print(f\"Success: {result.success}\")\n",
    "print(f\"Content length: {len(result.content)} chars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page Title: Casas y pisos en alquiler en Barcelona — idealista\n"
     ]
    }
   ],
   "source": [
    "# Check the page title\n",
    "soup = BeautifulSoup(result.content, \"html.parser\")\n",
    "title = soup.find(\"title\")\n",
    "print(f\"Page Title: {title.get_text() if title else 'N/A'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Find Listing Elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 30 listings with class 'item'\n"
     ]
    }
   ],
   "source": [
    "# Find all listing articles\n",
    "articles = soup.find_all(\"article\", class_=\"item\")\n",
    "print(f\"Found {len(articles)} listings with class 'item'\")\n",
    "\n",
    "if not articles:\n",
    "    # Try alternative selectors\n",
    "    articles = soup.find_all(\"article\")\n",
    "    print(f\"Total articles found: {len(articles)}\")\n",
    "    \n",
    "    for i, art in enumerate(articles[:5]):\n",
    "        classes = art.get(\"class\", [])\n",
    "        print(f\"  Article {i}: class={classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First listing HTML:\n",
      "------------------------------------------------------------\n",
      "<article class=\"item extended-item item-multimedia-container\" data-element-id=\"110429075\" data-online-booking=\"false\">\n",
      " <picture class=\"item-multimedia\">\n",
      "  <div class=\"item-multimedia-pictures\">\n",
      "   <div class=\"item-multimedia-pictures__container\">\n",
      "    <div class=\"item-multimedia-shortcuts --desktop\">\n",
      "     <button aria-label=\"Abrir mapa\" class=\"multimedia-shortcut icon-location-outline\" data-button-type=\"MAP\">\n",
      "     </button>\n",
      "    </div>\n",
      "    <span class=\"item-multimedia-pictures__ref-tag d-none\">\n",
      "    </span>\n",
      "    <div class=\"item-multimedia-pictures__counter\">\n",
      "     <span>\n",
      "      1/\n",
      "     </span>\n",
      "     <span>\n",
      "      19\n",
      "     </span>\n",
      "    </div>\n",
      "   </div>\n",
      "  </div>\n",
      "  <div class=\"item-ribbon-container\">\n",
      "  </div>\n",
      "  <div class=\"item-gallery gallery-height-core-vitals neutral-orientation\">\n",
      "   <div class=\"mask-wrapper is-clickable\">\n",
      "    <div class=\"mask\" style=\"touch-action: pan-y; user-select: none; -webkit-user-drag: none; -webkit-tap-highlight-color: rgba(0, 0, 0, 0); transition-duration: 0s; transform: translateX(0px);\">\n",
      "     <div class=\"placeholder\" style=\"transform: translateX(-100%);\">\n",
      "     </div>\n",
      "     <div class=\"placeholder\" style=\"transform: translateX(0px);\">\n",
      "      <picture>\n",
      "       <source height=\"360\" srcset=\"https://img4.idealista.com/blur/480_360_mq/0/id.pro.es.image.master/61/8d/f7/1404864434.webp\" type=\"image/webp\" width=\"480\"/>\n",
      "       <source height=\"360\" srcset=\"https://img4.idealista.com/blur/480_360_mq/0/id.pro.es.image.master/61/8d/f7/1404864434.jpg\" type=\"image/jpeg\" width=\"480\"/>\n",
      "       <img alt=\"Primera foto del inmueble\" height=\"360\" loading=\"lazy\" src=\"https://img4.idealista.com/blur/480_360_mq/0/id.pro.es.image.master/61/8d/f7/1404864434.jpg\" style=\"visibility: visible;\" width=\"480\"/>\n",
      "      </picture>\n",
      "     </div>\n",
      "     <div class=\"placeholder\" style=\"transform: translateX(100%);\">\n",
      "     </div>\n",
      "    </div>\n",
      "   </div>\n",
      "  </div>\n",
      "  <div class=\"item-multimedia-shortcuts --mobile\">\n",
      "   <button aria-label=\"Abrir mapa\" class=\"multimedia-shortcut icon-location-outline\" da\n"
     ]
    }
   ],
   "source": [
    "# Show first article HTML structure\n",
    "if articles:\n",
    "    print(\"First listing HTML:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(articles[0].prettify()[:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Parse Listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully parsed 30 listings\n"
     ]
    }
   ],
   "source": [
    "def parse_listing(article) -> dict:\n",
    "    \"\"\"Parse a single listing article.\"\"\"\n",
    "    data = {}\n",
    "    \n",
    "    # Link and title\n",
    "    link_elem = article.find(\"a\", class_=\"item-link\")\n",
    "    if link_elem:\n",
    "        href = link_elem.get(\"href\", \"\")\n",
    "        if not href.startswith(\"http\"):\n",
    "            href = \"https://www.idealista.com\" + href  # Spain domain\n",
    "        data[\"url\"] = href\n",
    "        data[\"title\"] = link_elem.get_text(strip=True)\n",
    "    \n",
    "    # Price\n",
    "    price_elem = article.find(\"span\", class_=\"item-price\")\n",
    "    data[\"price\"] = price_elem.get_text(strip=True) if price_elem else \"N/A\"\n",
    "    \n",
    "    # Details (rooms, size, floor)\n",
    "    details = article.find_all(\"span\", class_=\"item-detail\")\n",
    "    data[\"rooms\"] = details[0].get_text(strip=True) if len(details) > 0 else \"N/A\"\n",
    "    data[\"size\"] = details[1].get_text(strip=True) if len(details) > 1 else \"N/A\"\n",
    "    data[\"floor\"] = details[2].get_text(strip=True) if len(details) > 2 else \"N/A\"\n",
    "    \n",
    "    # Description\n",
    "    desc_elem = article.find(\"div\", class_=\"item-description\")\n",
    "    data[\"description\"] = desc_elem.get_text(strip=True) if desc_elem else \"\"\n",
    "    \n",
    "    # Thumbnail\n",
    "    img_elem = article.find(\"img\")\n",
    "    if img_elem:\n",
    "        data[\"thumbnail\"] = img_elem.get(\"src\") or img_elem.get(\"data-src\") or \"\"\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Parse all listings\n",
    "listings = []\n",
    "for article in articles:\n",
    "    try:\n",
    "        listing = parse_listing(article)\n",
    "        if listing.get(\"url\"):\n",
    "            listings.append(listing)\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing: {e}\")\n",
    "\n",
    "print(f\"Successfully parsed {len(listings)} listings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Listing 1\n",
      "============================================================\n",
      "Title: Piso en Calle de la Independència, El Camp de l'Arpa del Clot, Barcelona\n",
      "Price: 2.000€/mes\n",
      "Rooms: 3 hab.\n",
      "Size: 93 m²\n",
      "Floor: Planta 1ª exterior con ascensor\n",
      "URL: https://www.idealista.com/inmueble/110429075/\n",
      "\n",
      "============================================================\n",
      "Listing 2\n",
      "============================================================\n",
      "Title: Piso en Calle de Neptú, Vila de Gràcia, Barcelona\n",
      "Price: 1.350€/mes\n",
      "Rooms: 2 hab.\n",
      "Size: 70 m²\n",
      "Floor: Planta 2ª interior sin ascensor\n",
      "URL: https://www.idealista.com/inmueble/39923316/\n",
      "\n",
      "============================================================\n",
      "Listing 3\n",
      "============================================================\n",
      "Title: Piso en Calle del Peu de la Creu, El Raval, Barcelona\n",
      "Price: 1.295€/mes\n",
      "Rooms: 1 hab.\n",
      "Size: 91 m²\n",
      "Floor: Bajo exterior con ascensor\n",
      "URL: https://www.idealista.com/inmueble/110428966/\n",
      "\n",
      "============================================================\n",
      "Listing 4\n",
      "============================================================\n",
      "Title: Piso en Calle de Pere IV, 440, Provençals del Poblenou, Barcelona\n",
      "Price: 1.900€/mes\n",
      "Rooms: 1 hab.\n",
      "Size: 80 m²\n",
      "Floor: Bajo exterior con ascensor\n",
      "URL: https://www.idealista.com/inmueble/110378291/\n",
      "\n",
      "============================================================\n",
      "Listing 5\n",
      "============================================================\n",
      "Title: Ático en Calle de Berenguer Mallol, 99, La Barceloneta, Barcelona\n",
      "Price: 1.300€/mes\n",
      "Rooms: 3 hab.\n",
      "Size: 60 m²\n",
      "Floor: Planta 6ª exterior sin ascensor\n",
      "URL: https://www.idealista.com/inmueble/100129678/\n"
     ]
    }
   ],
   "source": [
    "# Display parsed listings\n",
    "for i, listing in enumerate(listings[:5]):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Listing {i+1}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Title: {listing.get('title', 'N/A')}\")\n",
    "    print(f\"Price: {listing.get('price', 'N/A')}\")\n",
    "    print(f\"Rooms: {listing.get('rooms', 'N/A')}\")\n",
    "    print(f\"Size: {listing.get('size', 'N/A')}\")\n",
    "    print(f\"Floor: {listing.get('floor', 'N/A')}\")\n",
    "    print(f\"URL: {listing.get('url', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Load/Save Seen Links"
  },
  {
   "cell_type": "code",
   "source": "# Seen links file path\nSEEN_LINKS_FILE = \"data/seen_links.txt\"\n\ndef load_seen_links() -> set:\n    \"\"\"Load seen URLs from text file.\"\"\"\n    seen = set()\n    try:\n        with open(SEEN_LINKS_FILE, \"r\", encoding=\"utf-8\") as f:\n            for line in f:\n                url = line.strip()\n                if url:\n                    seen.add(url)\n        print(f\"Loaded {len(seen)} seen URLs\")\n    except FileNotFoundError:\n        print(\"No seen links file yet, starting fresh\")\n    return seen\n\ndef save_seen_links(seen: set) -> None:\n    \"\"\"Save seen URLs to text file.\"\"\"\n    import os\n    os.makedirs(\"data\", exist_ok=True)\n    with open(SEEN_LINKS_FILE, \"w\", encoding=\"utf-8\") as f:\n        for url in seen:\n            f.write(url + \"\\n\")\n    print(f\"Saved {len(seen)} URLs to {SEEN_LINKS_FILE}\")\n\n# Load existing seen links\nseen_links = load_seen_links()\nprint(f\"Currently tracking {len(seen_links)} seen links\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Filter out already seen listings and add new ones to seen set\nnew_listings = []\nfor listing in listings:\n    url = listing.get(\"url\", \"\")\n    if url and url not in seen_links:\n        new_listings.append(listing)\n        seen_links.add(url)\n\nprint(f\"New listings (not seen before): {len(new_listings)}\")\nprint(f\"Already seen: {len(listings) - len(new_listings)}\")\n\n# Save updated seen links\nsave_seen_links(seen_links)\n\n# Save new listings as JSON for inspection\nwith open(\"debug_listings.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(new_listings, f, indent=2, ensure_ascii=False)\nprint(f\"Saved {len(new_listings)} new listings to debug_listings.json\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test Pagination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pagination found:\n",
      "<div class=\"pagination\">\n",
      " <ul>\n",
      "  <li class=\"moreresults\">\n",
      "   <span>\n",
      "    Ver más resultados:\n",
      "   </span>\n",
      "  </li>\n",
      "  <li class=\"selected\">\n",
      "   <span>\n",
      "    1\n",
      "   </span>\n",
      "  </li>\n",
      "  <li>\n",
      "   <a class=\"\" href=\"/alquiler-viviendas/barcelona-barcelona/con-precio-hasta_2400,precio-desde_1000/pagina-2.htm\" rel=\"nofollow\">\n",
      "    2\n",
      "   </a>\n",
      "  </li>\n",
      "  <li>\n",
      "   <a class=\"\" href=\"/alquiler-viviendas/barcelona-barcelona/con-precio-hasta_2400,precio-desde_1000/pagina-3.htm\" rel=\"nofollow\">\n",
      "    3\n",
      "   </a>\n",
      "  </li>\n",
      "  <li>\n",
      "   \n",
      "\n",
      "Next page: /alquiler-viviendas/barcelona-barcelona/con-precio-hasta_2400,precio-desde_1000/pagina-2.htm\n"
     ]
    }
   ],
   "source": [
    "# Check if there are more pages\n",
    "pagination = soup.find(\"div\", class_=\"pagination\")\n",
    "if pagination:\n",
    "    print(\"Pagination found:\")\n",
    "    print(pagination.prettify()[:500])\n",
    "else:\n",
    "    print(\"No pagination element found\")\n",
    "\n",
    "# Look for next page link\n",
    "next_link = soup.find(\"a\", class_=\"icon-arrow-right-after\")\n",
    "if next_link:\n",
    "    next_url = next_link.get(\"href\")\n",
    "    print(f\"\\nNext page: {next_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Scrape Multiple Pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching: https://www.idealista.com/alquiler-viviendas/barcelona-barcelona/con-precio-hasta_2400,precio-desde_1000/pagina-2.htm\n",
      "Found 30 listings on page 2\n"
     ]
    }
   ],
   "source": [
    "def scrape_page(url: str) -> list:\n",
    "    \"\"\"Scrape a single page and return listings.\"\"\"\n",
    "    result = scrapfly.scrape(\n",
    "        ScrapeConfig(\n",
    "            url=url,\n",
    "            asp=True,\n",
    "            country=\"ES\",  # Spain\n",
    "            render_js=True,\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    if not result.success:\n",
    "        print(f\"Failed: {result.upstream_status_code}\")\n",
    "        return []\n",
    "    \n",
    "    soup = BeautifulSoup(result.content, \"html.parser\")\n",
    "    articles = soup.find_all(\"article\", class_=\"item\")\n",
    "    \n",
    "    listings = []\n",
    "    for article in articles:\n",
    "        try:\n",
    "            listing = parse_listing(article)\n",
    "            if listing.get(\"url\"):\n",
    "                listings.append(listing)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return listings\n",
    "\n",
    "# Test with page 2\n",
    "# Format: https://www.idealista.com/.../filters/pagina-2.htm\n",
    "page2_url = SEARCH_URL.rstrip('/') + \"/pagina-2.htm\"\n",
    "print(f\"Fetching: {page2_url}\")\n",
    "\n",
    "page2_listings = scrape_page(page2_url)\n",
    "print(f\"Found {len(page2_listings)} listings on page 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. API Usage Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scrapfly Account Info:\n",
      "  Plan: N/A\n",
      "  Credits remaining: N/A\n"
     ]
    }
   ],
   "source": [
    "# Check your Scrapfly account usage\n",
    "try:\n",
    "    account = scrapfly.account()\n",
    "    print(\"Scrapfly Account Info:\")\n",
    "    print(f\"  Plan: {account.get('subscription', {}).get('plan', 'N/A')}\")\n",
    "    print(f\"  Credits remaining: {account.get('subscription', {}).get('remaining_credits', 'N/A')}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not fetch account info: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "for f in [\"debug_response.html\", \"debug_listings.json\"]:\n",
    "    if os.path.exists(f):\n",
    "        os.remove(f)\n",
    "        print(f\"Removed {f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}