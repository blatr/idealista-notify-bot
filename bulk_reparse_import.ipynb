{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Bulk re-parse URLs into To Be Communicated\n",
        "\n",
        "Paste a list of Idealista URLs below. This notebook re-scrapes each listing concurrently and upserts it into the DB with stage `to_be_communicated`. Existing rows are updated with fresh data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Input: one URL per line\n",
        "urls = [\n",
        "    # \"https://www.idealista.com/inmueble/12345678/\",\n",
        "]\n",
        "\n",
        "# Optional: cap concurrency\n",
        "max_workers = 5\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import asyncio\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from sqlalchemy import func\n",
        "from webapp.database.database import SessionLocal\n",
        "from webapp.database.models import Listing\n",
        "from webapp.services.scraper_service import parse_idealista_url\n",
        "\n",
        "STAGE_TO_BE_COMMUNICATED = \"to_be_communicated\"\n",
        "\n",
        "unique_urls = list(dict.fromkeys([u.strip() for u in urls if u and u.strip()]))\n",
        "if not unique_urls:\n",
        "    raise ValueError(\"No URLs provided\")\n",
        "\n",
        "def _parse_sync(url: str) -> dict:\n",
        "    # parse_idealista_url is async but internally uses a sync client;\n",
        "    # running it in a thread keeps the main loop responsive.\n",
        "    return asyncio.run(parse_idealista_url(url))\n",
        "\n",
        "def scrape_all(url_list, workers=5):\n",
        "    results = []\n",
        "    with ThreadPoolExecutor(max_workers=workers) as executor:\n",
        "        future_map = {executor.submit(_parse_sync, url): url for url in url_list}\n",
        "        for future in as_completed(future_map):\n",
        "            url = future_map[future]\n",
        "            try:\n",
        "                data = future.result()\n",
        "                results.append((url, data, None))\n",
        "            except Exception as exc:\n",
        "                results.append((url, None, exc))\n",
        "    return results\n",
        "\n",
        "print(f\"Scraping {len(unique_urls)} unique URLs with {max_workers} workers...\")\n",
        "scraped = scrape_all(unique_urls, workers=max_workers)\n",
        "\n",
        "db = SessionLocal()\n",
        "try:\n",
        "    existing = db.query(Listing).filter(Listing.idealista_url.in_(unique_urls)).all()\n",
        "    existing_map = {row.idealista_url: row for row in existing}\n",
        "\n",
        "    max_pos = db.query(func.max(Listing.position)).filter(\n",
        "        Listing.stage == STAGE_TO_BE_COMMUNICATED\n",
        "    ).scalar() or 0\n",
        "\n",
        "    created = 0\n",
        "    updated = 0\n",
        "    failed = 0\n",
        "\n",
        "    for url, data, err in scraped:\n",
        "        if err or not data:\n",
        "            failed += 1\n",
        "            print(f\"FAIL {url}: {err}\")\n",
        "            continue\n",
        "\n",
        "        row = existing_map.get(url)\n",
        "        if row:\n",
        "            for key, value in data.items():\n",
        "                setattr(row, key, value)\n",
        "            if row.stage != STAGE_TO_BE_COMMUNICATED:\n",
        "                max_pos += 1\n",
        "                row.stage = STAGE_TO_BE_COMMUNICATED\n",
        "                row.position = max_pos\n",
        "            row.source = \"bulk_import\"\n",
        "            updated += 1\n",
        "        else:\n",
        "            max_pos += 1\n",
        "            db.add(Listing(\n",
        "                title=data.get(\"title\"),\n",
        "                price=data.get(\"price\"),\n",
        "                price_value=data.get(\"price_value\"),\n",
        "                rooms=data.get(\"rooms\"),\n",
        "                size=data.get(\"size\"),\n",
        "                floor=data.get(\"floor\"),\n",
        "                description=data.get(\"description\"),\n",
        "                thumbnail=data.get(\"thumbnail\"),\n",
        "                idealista_url=data.get(\"idealista_url\") or url,\n",
        "                stage=STAGE_TO_BE_COMMUNICATED,\n",
        "                position=max_pos,\n",
        "                source=\"bulk_import\",\n",
        "            ))\n",
        "            created += 1\n",
        "\n",
        "    db.commit()\n",
        "    print(f\"Done. Created: {created}, Updated: {updated}, Failed: {failed}\")\n",
        "finally:\n",
        "    db.close()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
